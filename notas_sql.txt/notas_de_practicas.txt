

Lección 3 
Hacer tablas de costumers, products, 
Create table "table_name"( 
#darle todas las columnas, darle todos los tipo de datos por columna. 
"column 1" "data type for column 1" [], 
"column 2" "data type for column 2" [], 
[constraints]*opcional. acabar con semicolon ); 
Constraints existentes:
    NOT NULL: Ensures that a column cannot have NULL value
    DEFAULT: Provides a default value for a column when none is specified
    UNIQUE: Ensures all values in a column are different
    CHECK: Makes sure that all values in a column satisfy certain criteria. 
    Primary Key: Used to uniquely identify a row in the table
    Foreign Key: Used to ensure referential integrity of the data. 
Take personal: Ok los constraints son las notas o metadatos sobre las columnas. Que condiciones o tipos de datos generales tiene la tabla.
tip: verbos en mayúsculas son mejores para diferenciar los queries. 

    PRIMARY Key. 
    Identifica de manera única cada fila en una tabla. 
    Pueden ser una o más columnas, si más de una se conoce como clave compuesta. 
    Foreign key. 
    Identifica una columna de otra tabla (muy frecuentemente su clave primaria)
    El objetivo de las claves foraneas es garantizar integridad de referencialidad de los datos. 

Ejemplo
Bueno para hacer la tabla se va a la base de datos click derecho query tool: 
    ejemplo: 
    CREATE table customer_table(
Cust_id int, 
First_name varchar, 
Last_name varchar, 
age int, 
email_id varchar);
y para revisarla se va a Schemas, tables y ahí aparece registrada. 

Equivalente en Big Query 
INT64 (en lugar de int)
FLOAT64, NUMERIC, BOOL,
STRING, DATE, DATETIME
no hay constraints (no primary key, foreign key, unique)

Create a new project in Big Query 
Botón New Project > Nombrar > crear nuevo conjunto de datos > nombrar > crear tabla o desde query >
ejemplo de query: 
 create table DB_1.customer_table (
  cust_id int64, 
  First_name string, 
  Last_name string, 
  age int64,
  email string
);

Caso práctico: 
Crear una base de datos "Classroom" con una tabla "Science_class" con 3 columnas y sus tipos de datos. 
Query: 
CREATE TABLE Science_class(
Enrollment_no Int, 
Name Varchar, 
Science_Marks int
); 

INSERT INTO 
    Sintaxis
        INSERT INTO "table_name"("column1", "column2", ...)
        VALUES ("value1", "value2", ...)
    
Single Row 
INSERT INTO customer_table
VALUES (1, 'bee', 'cee', 32, 'bc@xyz.com'); #se pueden especificar nombres (cust_id, first_name, age, email_id) #no sirve doble comillas.

Multiple Rows 
INSERT INTO customer_table 
VALUES (3, 'ee', 'ef', 27, 'ef@xyz.com'),
(1, 'gee', 'eh', 42, 'gh@xyz.com'), 
...; 

Import data from files 
COPY "table_name" ("column1", "column2", ...)
FROM 'C:\tmp\persons.csv' Delimiter ',' CSV HEADER;

#Se hace práctica usando el folder resources #pedirmelo a mi 
query ejemplo: 
COPY customer_table from 
'C:\Users\david\Documents\main\Assets\Google-BigQuery-PostgreSQL-_-Big-Query-for-Data-Analysis\Data\Data_copy\copy.csv' delimiter ',' csv header;
    Luego revisar en tablas, click derecho, ver, ver todos los registros

#Noté que no hubo resultados añadidos arreglaré la ruta cambiando la dirección de los slashes
"C:/Users/Public/Documents/PostgresData/postgresql_data_resources/Data/Data_copy/copy.csv"
#tuve que poner en una carpeta pública para que postgres tuviera acceso. No podía acceder a carpetas de usuario. 

BigQuery
No hay copy statement. 
    Google biquery o google cloud shell. 

Se selecciona la base de datos, crear tabla, upload, browse file, 
    en que proyecto base de datos tabla o nombrar tabla, hay opcion auto detectar columnas y tipos en schema, 
        en opciones avanzadas hay detalles como escribir si vacio hacer un apendice o sobreescribir.
            Header rows to skip 1. Porque sino trata como dato los nombres de las columnas. 
#si algun tipo de datos no es constante generara un error. Se puede hacer una excepcion de cantidad de errores. La data debe estar limpia *buenas prácticas
Recordar revisar el schema ya que usualmente se usa auto. Sino pues Edit Schema (button) y cambiarlo. 

    constraints de cargar datos locales
        Tamaño: menos de 10 mb
        Filas: menos de 16K. 
            Si no: 
                Cargar desde drive o google cloud shell

Archivos grandes
    Descargar google cloud sdk 
        log in desde google 
            seleccionar proyecto en nube (nombre de proyectos en bigquery el display gráfico es una terminal sdk shell)
                bq --skip_leading_rows=1 load table_name (ejemplo: DB_1.customer_table) copy.csv (name of file)

Importing from google drive 
    Create table > Drive > URL > 
        Ventajas: no pagas por el almacenamiento por que es datos que tienes en el drive. 
#Ejercicio de importación e insertado. 
